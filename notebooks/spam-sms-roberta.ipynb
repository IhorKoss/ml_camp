{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":982,"sourceType":"datasetVersion","datasetId":483},{"sourceId":260807,"sourceType":"datasetVersion","datasetId":109196}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-01T15:23:36.144721Z","iopub.execute_input":"2025-02-01T15:23:36.144990Z","iopub.status.idle":"2025-02-01T15:23:36.445752Z","shell.execute_reply.started":"2025-02-01T15:23:36.144960Z","shell.execute_reply":"2025-02-01T15:23:36.444959Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/sms-spam-collection-dataset/spam.csv\n/kaggle/input/spam-mails-dataset/spam_ham_dataset.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\nimport torch\nfrom datasets import load_dataset, concatenate_datasets\nfrom transformers import (\n    RobertaTokenizerFast,\n    RobertaForSequenceClassification,\n    DataCollatorWithPadding,\n    TrainingArguments,\n    Trainer,\n    AutoConfig,\n    TextClassificationPipeline,\n    AutoModelForSequenceClassification,\n    AutoTokenizer\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T19:25:34.531714Z","iopub.execute_input":"2025-01-18T19:25:34.532059Z","iopub.status.idle":"2025-01-18T19:25:49.149952Z","shell.execute_reply.started":"2025-01-18T19:25:34.532025Z","shell.execute_reply":"2025-01-18T19:25:49.149294Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"sms_file_path = '/kaggle/input/sms-spam-collection-dataset/spam.csv'\nloaded_ds = load_dataset(\"SetFit/enron_spam\", split=None)\nconcatenated_ds = concatenate_datasets([loaded_ds[\"train\"], loaded_ds[\"test\"]])\nto_pandas_ds = pd.DataFrame(concatenated_ds)\nto_pandas_ds=to_pandas_ds.drop(to_pandas_ds.columns[[0,3,4,5,6]], axis=1)\ndf_2 = to_pandas_ds.sample(frac=0.5, random_state=42)\n\n\ndf = pd.read_csv(sms_file_path, encoding='latin-1')\ndf=df.drop(df.columns[[2,3,4]], axis=1)\ndf['spam'] = df['v1'].apply(lambda x: 1 if x == 'spam' else 0)\ndf=df.drop(df.columns[[0]], axis=1)\ndf.columns = ['text', 'label']\nfinal_df = pd.concat([df, df_2], axis=0, ignore_index=True)\n\nprint(final_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T19:48:04.996933Z","iopub.execute_input":"2025-01-17T19:48:04.997208Z","iopub.status.idle":"2025-01-17T19:48:07.930131Z","shell.execute_reply.started":"2025-01-17T19:48:04.997186Z","shell.execute_reply":"2025-01-17T19:48:07.929395Z"}},"outputs":[{"name":"stderr","text":"Repo card metadata block was not found. Setting CardData to empty.\n","output_type":"stream"},{"name":"stdout","text":"                                                    text  label\n0      Go until jurong point, crazy.. Available only ...      0\n1                          Ok lar... Joking wif u oni...      0\n2      Free entry in 2 a wkly comp to win FA Cup fina...      1\n3      U dun say so early hor... U c already then say...      0\n4      Nah I don't think he goes to usf, he lives aro...      0\n...                                                  ...    ...\n22425  agenda : ubs warburg / \" energy \" integration ...      0\n22426  your file sleeps around man cheating -\\nstart ...      1\n22427  it works greatt hello , welcome to medzonli mo...      1\n22428  are you a penny stox player ? mnei - the best ...      1\n22429  re [ 10 ] the biggest tit b / \\ bes | n the wo...      1\n\n[22430 rows x 2 columns]\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T19:26:10.123358Z","iopub.execute_input":"2025-01-18T19:26:10.123750Z","iopub.status.idle":"2025-01-18T19:26:11.103784Z","shell.execute_reply.started":"2025-01-18T19:26:10.123717Z","shell.execute_reply":"2025-01-18T19:26:11.102741Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d754d7c5c77845f7bea7a64bfb7127a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0359b7f0db4545728e511f7b25b4ee8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14fc53a597e343598777a81d457a73f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a89de6e2308641eb8ce912b7d9942064"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0f99f9d9a58467e89ee20c27ad6631d"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"X = final_df[\"text\"]\ny=final_df[\"label\"]\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T19:48:18.253313Z","iopub.execute_input":"2025-01-17T19:48:18.253632Z","iopub.status.idle":"2025-01-17T19:48:18.263984Z","shell.execute_reply.started":"2025-01-17T19:48:18.253608Z","shell.execute_reply":"2025-01-17T19:48:18.263206Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from datasets import Dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T19:32:32.671763Z","iopub.execute_input":"2025-01-18T19:32:32.672071Z","iopub.status.idle":"2025-01-18T19:32:32.675824Z","shell.execute_reply.started":"2025-01-18T19:32:32.672047Z","shell.execute_reply":"2025-01-18T19:32:32.674996Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"train_dataset = Dataset.from_pandas(pd.DataFrame({\"text\": X_train, \"label\": y_train}))\nval_dataset = Dataset.from_pandas(pd.DataFrame({\"text\": X_val, \"label\": y_val}))\ntest_dataset = Dataset.from_pandas(pd.DataFrame({\"text\": X_test, \"label\": y_test}))\n\ndef tokenize(batch):\n    return tokenizer(batch[\"text\"], padding=True, truncation=True, max_length=256)\n\ntrain_dataset = train_dataset.map(tokenize, batched=True)\nval_dataset = val_dataset.map(tokenize, batched=True)\ntest_dataset = test_dataset.map(tokenize, batched=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T19:48:24.061808Z","iopub.execute_input":"2025-01-17T19:48:24.062087Z","iopub.status.idle":"2025-01-17T19:48:34.397140Z","shell.execute_reply.started":"2025-01-17T19:48:24.062067Z","shell.execute_reply":"2025-01-17T19:48:34.396296Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/15701 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"394c857cbb2940a091ec440e5859bdd4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3364 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14c35a2c24de4558ba02fec359eb0731"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3365 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb9abdf0669b4801b6f440208c0e87e2"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\nval_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\ntest_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T19:48:40.970371Z","iopub.execute_input":"2025-01-17T19:48:40.970647Z","iopub.status.idle":"2025-01-17T19:48:40.976642Z","shell.execute_reply.started":"2025-01-17T19:48:40.970629Z","shell.execute_reply":"2025-01-17T19:48:40.975998Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"num_labels = 2  \nclass_names = [\"ham\", \"spam\"]\nprint(f\"number of labels: {num_labels}\")\nprint(f\"the labels: {class_names}\")\n\nid2label = {0: \"ham\", 1: \"spam\"}\n\nconfig = AutoConfig.from_pretrained(\"roberta-base\")\nconfig.update({\"id2label\": id2label})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T19:48:42.946539Z","iopub.execute_input":"2025-01-17T19:48:42.946842Z","iopub.status.idle":"2025-01-17T19:48:42.995061Z","shell.execute_reply.started":"2025-01-17T19:48:42.946817Z","shell.execute_reply":"2025-01-17T19:48:42.994434Z"}},"outputs":[{"name":"stdout","text":"number of labels: 2\nthe labels: ['ham', 'spam']\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = logits.argmax(axis=-1)\n    return {\n        \"f1\": f1_score(labels, predictions, average=\"binary\"),\n        \"accuracy\": accuracy_score(labels, predictions),\n        \"precision\": precision_score(labels, predictions, average=\"binary\"),\n        \"recall\": recall_score(labels, predictions, average=\"binary\"),\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T19:48:45.090111Z","iopub.execute_input":"2025-01-17T19:48:45.090436Z","iopub.status.idle":"2025-01-17T19:48:45.094696Z","shell.execute_reply.started":"2025-01-17T19:48:45.090411Z","shell.execute_reply":"2025-01-17T19:48:45.093965Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"model = RobertaForSequenceClassification.from_pretrained('roberta-base',config=config)\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\ntraining_args = TrainingArguments(\n    output_dir=\"output\",\n    num_train_epochs=5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    evaluation_strategy=\"epoch\",\n    logging_dir=\"./logs\",\n    logging_strategy=\"steps\",\n    logging_steps=10,\n    learning_rate=5e-5,\n    weight_decay=0.01,\n    warmup_steps=250,\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    report_to=\"tensorboard\",\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T19:48:48.488520Z","iopub.execute_input":"2025-01-17T19:48:48.488800Z","iopub.status.idle":"2025-01-17T19:48:51.497035Z","shell.execute_reply.started":"2025-01-17T19:48:48.488777Z","shell.execute_reply":"2025-01-17T19:48:51.496414Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a36f33b448a145b2aa58e4cf9ce267f4"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T19:48:58.713185Z","iopub.execute_input":"2025-01-17T19:48:58.713516Z","iopub.status.idle":"2025-01-17T20:34:06.218549Z","shell.execute_reply.started":"2025-01-17T19:48:58.713491Z","shell.execute_reply":"2025-01-17T20:34:06.217849Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4910' max='4910' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4910/4910 45:05, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.031500</td>\n      <td>0.101173</td>\n      <td>0.972779</td>\n      <td>0.977408</td>\n      <td>0.979798</td>\n      <td>0.965861</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.042800</td>\n      <td>0.076403</td>\n      <td>0.980378</td>\n      <td>0.983650</td>\n      <td>0.983536</td>\n      <td>0.977240</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.039100</td>\n      <td>0.065416</td>\n      <td>0.986856</td>\n      <td>0.989001</td>\n      <td>0.985806</td>\n      <td>0.987909</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.000100</td>\n      <td>0.075085</td>\n      <td>0.987892</td>\n      <td>0.989893</td>\n      <td>0.989301</td>\n      <td>0.986486</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.000100</td>\n      <td>0.077640</td>\n      <td>0.988260</td>\n      <td>0.990190</td>\n      <td>0.988612</td>\n      <td>0.987909</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=4910, training_loss=0.05256599639806766, metrics={'train_runtime': 2707.0586, 'train_samples_per_second': 29.0, 'train_steps_per_second': 1.814, 'total_flos': 1.03277667005184e+16, 'train_loss': 0.05256599639806766, 'epoch': 5.0})"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"trainer.evaluate()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T20:34:10.132972Z","iopub.execute_input":"2025-01-17T20:34:10.133277Z","iopub.status.idle":"2025-01-17T20:34:47.138069Z","shell.execute_reply.started":"2025-01-17T20:34:10.133229Z","shell.execute_reply":"2025-01-17T20:34:47.137334Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 0.06541559100151062,\n 'eval_f1': 0.9868561278863234,\n 'eval_accuracy': 0.989001189060642,\n 'eval_precision': 0.985805535841022,\n 'eval_recall': 0.9879089615931721,\n 'eval_runtime': 36.9957,\n 'eval_samples_per_second': 90.929,\n 'eval_steps_per_second': 5.703,\n 'epoch': 5.0}"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"test_results = trainer.predict(test_dataset)\n\nprint(test_results.metrics)\n\npredictions = test_results.predictions\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T20:35:27.450807Z","iopub.execute_input":"2025-01-17T20:35:27.451089Z","iopub.status.idle":"2025-01-17T20:36:04.656443Z","shell.execute_reply.started":"2025-01-17T20:35:27.451067Z","shell.execute_reply":"2025-01-17T20:36:04.655716Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"name":"stdout","text":"{'test_loss': 0.07663601636886597, 'test_f1': 0.9828203292770222, 'test_accuracy': 0.9857355126300149, 'test_precision': 0.9856424982053122, 'test_recall': 0.9800142755174875, 'test_runtime': 37.1981, 'test_samples_per_second': 90.462, 'test_steps_per_second': 5.672}\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"pipeline = TextClassificationPipeline(model=model, tokenizer=tokenizer,device=0)\nmessage='Todays Vodafone numbers ending with 4882 are selected to a receive a £350 award. If your number matches call 09064019014 to receive your £350 award.'\nresult = pipeline(message)\nprint(result)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T20:36:19.548847Z","iopub.execute_input":"2025-01-17T20:36:19.549115Z","iopub.status.idle":"2025-01-17T20:36:19.567422Z","shell.execute_reply.started":"2025-01-17T20:36:19.549095Z","shell.execute_reply":"2025-01-17T20:36:19.566509Z"}},"outputs":[{"name":"stdout","text":"[{'label': 'spam', 'score': 0.9991759657859802}]\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T18:24:58.274372Z","iopub.execute_input":"2025-01-18T18:24:58.274750Z","iopub.status.idle":"2025-01-18T18:24:58.298092Z","shell.execute_reply.started":"2025-01-18T18:24:58.274720Z","shell.execute_reply":"2025-01-18T18:24:58.297166Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf51db188fde45408a651c7578ea90af"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"trainer.push_to_hub(\"roberta_email_sms_spam_classifier\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T20:40:55.641197Z","iopub.execute_input":"2025-01-17T20:40:55.641510Z","iopub.status.idle":"2025-01-17T20:41:16.299838Z","shell.execute_reply.started":"2025-01-17T20:40:55.641488Z","shell.execute_reply":"2025-01-17T20:41:16.299163Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"training_args.bin:   0%|          | 0.00/5.11k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a6f32be9a46458e8a9d74bd569f164a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a4571fb5f524078a25c98e009c89981"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"face3427ebc44cfd9e94ec05c1cac752"}},"metadata":{}},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/coconutsc/output/commit/9316406e3df941878670276bda9579576d130326', commit_message='roberta_email_sms_spam_classifier', commit_description='', oid='9316406e3df941878670276bda9579576d130326', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, AutoTokenizer, RobertaTokenizerFast, TextClassificationPipeline","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T15:23:45.837295Z","iopub.execute_input":"2025-02-01T15:23:45.837621Z","iopub.status.idle":"2025-02-01T15:23:58.606114Z","shell.execute_reply.started":"2025-02-01T15:23:45.837591Z","shell.execute_reply":"2025-02-01T15:23:58.605450Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"loaded_model = AutoModelForSequenceClassification.from_pretrained(\"coconutsc/roberta_email_sms_spam_classifier\")\n# tokenizer = AutoTokenizer.from_pretrained(\"coconutsc/output\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T15:23:59.653433Z","iopub.execute_input":"2025-02-01T15:23:59.653983Z","iopub.status.idle":"2025-02-01T15:24:13.051086Z","shell.execute_reply.started":"2025-02-01T15:23:59.653953Z","shell.execute_reply":"2025-02-01T15:24:13.050431Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/788 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1839b1eb134940cc8b58d4d264ed2537"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9eb3838f6ff443195522073573e5938"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\npipeline = TextClassificationPipeline(model=loaded_model, tokenizer=tokenizer,device=0)\nmessage='hi how are you'\nresult = pipeline(message)\nprint(result)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T15:24:50.087406Z","iopub.execute_input":"2025-02-01T15:24:50.087834Z","iopub.status.idle":"2025-02-01T15:24:50.266254Z","shell.execute_reply.started":"2025-02-01T15:24:50.087794Z","shell.execute_reply":"2025-02-01T15:24:50.265389Z"}},"outputs":[{"name":"stdout","text":"[{'label': 'spam', 'score': 0.9870651960372925}]\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"test_set_path = '/kaggle/input/spam-mails-dataset/spam_ham_dataset.csv'\ndf_test = pd.read_csv(test_set_path)\ndf_test=df_test.drop(df_test.columns[[0,1]], axis=1)\ndf_test.columns = ['text', 'label']\n# df_test = df_test.sample(frac=0.5, random_state=42)\nprint(df_test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T12:20:30.974590Z","iopub.execute_input":"2025-02-01T12:20:30.974888Z","iopub.status.idle":"2025-02-01T12:20:31.105908Z","shell.execute_reply.started":"2025-02-01T12:20:30.974864Z","shell.execute_reply":"2025-02-01T12:20:31.105175Z"}},"outputs":[{"name":"stdout","text":"                                                   text  label\n0     Subject: enron methanol ; meter # : 988291\\r\\n...      0\n1     Subject: hpl nom for january 9 , 2001\\r\\n( see...      0\n2     Subject: neon retreat\\r\\nho ho ho , we ' re ar...      0\n3     Subject: photoshop , windows , office . cheap ...      1\n4     Subject: re : indian springs\\r\\nthis deal is t...      0\n...                                                 ...    ...\n5166  Subject: put the 10 on the ft\\r\\nthe transport...      0\n5167  Subject: 3 / 4 / 2000 and following noms\\r\\nhp...      0\n5168  Subject: calpine daily gas nomination\\r\\n>\\r\\n...      0\n5169  Subject: industrial worksheets for august 2000...      0\n5170  Subject: important online banking alert\\r\\ndea...      1\n\n[5171 rows x 2 columns]\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"results = pipeline(df_test[\"text\"].tolist(), batch_size=16,padding=True,truncation=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T12:20:33.510222Z","iopub.execute_input":"2025-02-01T12:20:33.510577Z","iopub.status.idle":"2025-02-01T12:23:26.880647Z","shell.execute_reply.started":"2025-02-01T12:20:33.510549Z","shell.execute_reply":"2025-02-01T12:23:26.879703Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"df_test[\"predicted_label\"] = [1 if res[\"label\"] == \"spam\" else 0 for res in results]\nprint(df_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T12:23:29.758687Z","iopub.execute_input":"2025-02-01T12:23:29.758994Z","iopub.status.idle":"2025-02-01T12:23:29.769068Z","shell.execute_reply.started":"2025-02-01T12:23:29.758967Z","shell.execute_reply":"2025-02-01T12:23:29.768122Z"}},"outputs":[{"name":"stdout","text":"                                                   text  label  \\\n0     Subject: enron methanol ; meter # : 988291\\r\\n...      0   \n1     Subject: hpl nom for january 9 , 2001\\r\\n( see...      0   \n2     Subject: neon retreat\\r\\nho ho ho , we ' re ar...      0   \n3     Subject: photoshop , windows , office . cheap ...      1   \n4     Subject: re : indian springs\\r\\nthis deal is t...      0   \n...                                                 ...    ...   \n5166  Subject: put the 10 on the ft\\r\\nthe transport...      0   \n5167  Subject: 3 / 4 / 2000 and following noms\\r\\nhp...      0   \n5168  Subject: calpine daily gas nomination\\r\\n>\\r\\n...      0   \n5169  Subject: industrial worksheets for august 2000...      0   \n5170  Subject: important online banking alert\\r\\ndea...      1   \n\n      predicted_label  \n0                   0  \n1                   0  \n2                   0  \n3                   1  \n4                   0  \n...               ...  \n5166                0  \n5167                0  \n5168                0  \n5169                0  \n5170                1  \n\n[5171 rows x 3 columns]\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\ntrue_labels = df_test[\"label\"]\n\naccuracy = accuracy_score(true_labels, df_test[\"predicted_label\"])\nprecision = precision_score(true_labels, df_test[\"predicted_label\"])\nrecall = recall_score(true_labels, df_test[\"predicted_label\"])\nf1 = f1_score(true_labels, df_test[\"predicted_label\"])\n\nprint(f\"Accuracy: {accuracy}\")\nprint(f\"Precision: {precision}\")\nprint(f\"Recall: {recall}\")\nprint(f\"F11 Score: {f1}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T12:23:48.750956Z","iopub.execute_input":"2025-02-01T12:23:48.751293Z","iopub.status.idle":"2025-02-01T12:23:48.769340Z","shell.execute_reply.started":"2025-02-01T12:23:48.751242Z","shell.execute_reply":"2025-02-01T12:23:48.768686Z"}},"outputs":[{"name":"stdout","text":"Accuracy: 0.9696383678205376\nPrecision: 0.9955686853766618\nRecall: 0.8992661774516344\nF11 Score: 0.9449702067998597\n","output_type":"stream"}],"execution_count":16}]}